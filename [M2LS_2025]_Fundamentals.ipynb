{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krivi12/M2L_school/blob/main/%5BM2LS_2025%5D_Fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdptlndy2eqn"
      },
      "source": [
        "## Setup\n",
        "\n",
        "---\n",
        "\n",
        "Additional Notes and Setup (framework info, python imports etc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOnGQ_LYQuwk",
        "outputId": "118ac649-d351-46e1-c98d-f87acf524288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yPuj1uwmQZyT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pk1cGnMuQZyU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import multivariate_normal\n",
        "import matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-5D01Ha9QZyU"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvY-GbhGQZyU"
      },
      "source": [
        "### Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPgsJ8DgQZyU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "import matplotlib\n",
        "\n",
        "# Parameters\n",
        "n_samples = 50000\n",
        "noise = 0.005\n",
        "random_state = 42\n",
        "\n",
        "# Generate data\n",
        "X, y = make_moons(\n",
        "    n_samples=n_samples,\n",
        "    noise=noise,\n",
        "    random_state=random_state\n",
        ")\n",
        "\n",
        "# Center and scale X\n",
        "mu = X.mean(axis=0)\n",
        "std = X.std(axis=0)\n",
        "X = (X - mu) / std\n",
        "\n",
        "\n",
        "def plot_dataset():\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Pick two discrete colors from viridis\n",
        "    cmap = matplotlib.colormaps[\"viridis\"].resampled(2)  # 2 discrete colors\n",
        "    colors = [cmap(0), cmap(1)]\n",
        "\n",
        "    # Plot each class with its viridis color\n",
        "    ax.scatter(X[y == 0, 0], X[y == 0, 1], c=[colors[0]], s=12, label=\"0\")\n",
        "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c=[colors[1]], s=12, label=\"1\")\n",
        "\n",
        "    ax.set_title(\"Two Moons\")\n",
        "    ax.set_xlabel(\"x\")\n",
        "    ax.set_ylabel(\"y\")\n",
        "\n",
        "    # Set the plot limits\n",
        "    ax.set_xlim(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5)\n",
        "    ax.set_ylim(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5)\n",
        "\n",
        "    # Add a simple legend\n",
        "    ax.legend(title=\"Label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUHnpyV_QZyV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_forward(X, y, alpha, sigma):\n",
        "    \"\"\"\n",
        "    Visualize the VP-SDE forward process.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): Initial data of shape (N, 2)\n",
        "        y (np.ndarray): Labels for coloring the scatter plot\n",
        "        alpha (callable): Function alpha(t)\n",
        "        sigma (callable): Function sigma(t)\n",
        "    \"\"\"\n",
        "    # Convert numpy array to torch tensor\n",
        "    X_tensor = torch.from_numpy(X).float()\n",
        "\n",
        "    # Define the timesteps we want to visualize\n",
        "    timesteps = torch.tensor(\n",
        "        [0.0, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5, 0.75, 1.0])\n",
        "\n",
        "    # Determine the number of rows and columns for the subplots\n",
        "    num_timesteps = len(timesteps) + 1  # include final Normal(0,1)\n",
        "    num_cols = 4\n",
        "    num_rows = (num_timesteps + num_cols - 1) // num_cols\n",
        "\n",
        "    # Set up the plot\n",
        "    fig, axes = plt.subplots(\n",
        "        num_rows, num_cols, figsize=(5 * num_cols, 5 * num_rows))\n",
        "    fig.suptitle(\"Forward Process\", fontsize=16)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Forward process for each timestep\n",
        "    for i, t in enumerate(timesteps):\n",
        "        alpha_t = alpha(t)\n",
        "        sigma_t = sigma(t)\n",
        "        epsilon = torch.randn_like(X_tensor)\n",
        "        X_t = alpha_t * X_tensor + sigma_t * epsilon\n",
        "\n",
        "        ax = axes[i]\n",
        "        ax.scatter(X_t[:, 0].numpy(), X_t[:, 1].numpy(),\n",
        "                   s=10, c=y, cmap='viridis')\n",
        "        ax.set_title(f\"t = {t.item():.2f}\")\n",
        "        ax.set_xlabel(\"x\")\n",
        "        ax.set_ylabel(\"y\")\n",
        "        ax.set_xlim(-3, 3)\n",
        "        ax.set_ylim(-3, 3)\n",
        "        ax.grid(True)\n",
        "\n",
        "    # Final Normal(0,1) plot\n",
        "    epsilon = torch.randn_like(X_tensor)\n",
        "    final_ax = axes[num_timesteps - 1]\n",
        "    final_ax.scatter(epsilon[:, 0].numpy(), epsilon[:, 1].numpy(), s=10)\n",
        "    final_ax.set_title(\"Normal (0,1)\")\n",
        "    final_ax.set_xlabel(\"x\")\n",
        "    final_ax.set_ylabel(\"y\")\n",
        "    final_ax.set_xlim(-3, 3)\n",
        "    final_ax.set_ylim(-3, 3)\n",
        "    final_ax.grid(True)\n",
        "\n",
        "    # Turn off any unused subplots\n",
        "    for j in range(num_timesteps, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_schedule(alpha_fn, beta_fn, sigma_fn, t=None):\n",
        "    \"\"\"\n",
        "    Plot beta(t), alpha(t), and sigma(t) side by side for t in [0,1].\n",
        "\n",
        "    Args:\n",
        "        alpha_fn: function t -> alpha(t)\n",
        "        beta_fn: function t -> beta(t)\n",
        "        sigma_fn: function t -> sigma(t)\n",
        "        t: optional torch.Tensor of time values, default linspace(0,1,500)\n",
        "    \"\"\"\n",
        "    if t is None:\n",
        "        t = torch.linspace(0, 1, 500)\n",
        "\n",
        "    b = beta_fn(t)\n",
        "    a = alpha_fn(t)\n",
        "    s = sigma_fn(t)\n",
        "\n",
        "    cmap = plt.get_cmap(\"viridis\")\n",
        "    colors = [cmap(0.2), cmap(0.5), cmap(0.8)]\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "    axs[0].plot(t, b, color=colors[0])\n",
        "    axs[0].set_title(\"beta(t)\")\n",
        "    axs[0].set_xlabel(\"t\")\n",
        "    axs[0].set_ylabel(\"beta\")\n",
        "\n",
        "    axs[1].plot(t, a, color=colors[1])\n",
        "    axs[1].set_title(\"alpha(t)\")\n",
        "    axs[1].set_xlabel(\"t\")\n",
        "    axs[1].set_ylabel(\"alpha\")\n",
        "\n",
        "    axs[2].plot(t, s, color=colors[2])\n",
        "    axs[2].set_title(\"sigma(t)\")\n",
        "    axs[2].set_xlabel(\"t\")\n",
        "    axs[2].set_ylabel(\"sigma\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik9_qu_kQZyV"
      },
      "outputs": [],
      "source": [
        "def plot_denoising_trajectories(traj: np.ndarray, ground_truth_samples: np.ndarray, num_samples_to_plot: int = 4):\n",
        "    \"\"\"\n",
        "    Plots the denoising trajectories from a standard Gaussian to a ground truth distribution.\n",
        "\n",
        "    Args:\n",
        "        traj (np.ndarray): A 3D numpy array of shape (num_trajectories, num_steps, 2)\n",
        "                           representing the trajectories.\n",
        "        ground_truth_samples (np.ndarray): A 2D numpy array of shape (num_samples, 2)\n",
        "                                           representing the ground truth data points.\n",
        "        num_samples_to_plot (int, optional): The number of trajectories to plot. Defaults to 4.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "    # 1. Ground truth (black background points, p0)\n",
        "    ax.scatter(ground_truth_samples[:, 0], ground_truth_samples[:,\n",
        "               1], c=\"black\", s=3, label=r\"$p_0$ (Ground truth)\")\n",
        "\n",
        "    # Pick colors from viridis\n",
        "    cmap = matplotlib.colormaps[\"viridis\"]\n",
        "    colors = [cmap(i / max(1, num_samples_to_plot - 1))\n",
        "              for i in range(num_samples_to_plot)]\n",
        "\n",
        "    # 2. Plot trajectories, noises, and samples\n",
        "    for i in range(num_samples_to_plot):\n",
        "        color = colors[i]\n",
        "\n",
        "        # Trajectory (light gray line)\n",
        "        ax.plot(traj[i, :, 0], traj[i, :, 1],\n",
        "                color=\"gray\", alpha=0.2, linewidth=1)\n",
        "\n",
        "        # Arrow (stronger gray)\n",
        "        x0, y0 = traj[i, 0]\n",
        "        x1, y1 = traj[i, -1]\n",
        "        ax.annotate(\"\",\n",
        "                    xy=(x1, y1), xycoords='data',\n",
        "                    xytext=(x0, y0), textcoords='data',\n",
        "                    arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=1.2, alpha=0.6))\n",
        "\n",
        "        # Initial noise (X marker)\n",
        "        ax.scatter(x0, y0, c=[color], s=60, marker=\"x\", linewidths=2,\n",
        "                   label=\"Initial noise\" if i == 0 else \"\")\n",
        "\n",
        "        # Final sample (O marker)\n",
        "        ax.scatter(x1, y1, c=[color], s=60, marker=\"o\",  linewidths=1.2,\n",
        "                   label=\"Final sample\" if i == 0 else \"\")\n",
        "\n",
        "    # 3. Gaussian contours (p1)\n",
        "    xx, yy = np.meshgrid(\n",
        "        np.linspace(-3, 3, 200),\n",
        "        np.linspace(-3, 3, 200)\n",
        "    )\n",
        "    pos = np.dstack((xx, yy))\n",
        "    rv = multivariate_normal([0, 0], [[1, 0], [0, 1]])\n",
        "    z = rv.pdf(pos)\n",
        "    ax.contour(xx, yy, z, colors=\"gray\", linestyles=\"dotted\")\n",
        "\n",
        "    # --- Legend entries for non-automatic elements ---\n",
        "    # p1 (Gaussian)\n",
        "    ax.plot([], [], color=\"gray\", linestyle=\"dotted\",\n",
        "            label=r\"$p_1$ (Standard Gaussian)\")\n",
        "    # Sampling trajectory\n",
        "    ax.plot([], [], color=\"gray\", alpha=0.2,\n",
        "            linewidth=1, label=\"Sampling trajectory\")\n",
        "\n",
        "    ax.axis(\"equal\")\n",
        "    ax.set_title(\"Denoising using Reverse Process\")\n",
        "    ax.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRqAb3dZQZyV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "\n",
        "def animate_trajectories(traj, downsample_points=2, downsample_frames=5, gif_path=\"denoising.gif\"):\n",
        "    \"\"\"\n",
        "    Animate a trajectory tensor and optionally save as GIF.\n",
        "\n",
        "    Args:\n",
        "        traj (np.ndarray): Trajectories of shape (num_points, num_timesteps, 2)\n",
        "        downsample_points (int): Factor to downsample points for faster animation\n",
        "        downsample_frames (int): Factor to downsample frames for faster animation\n",
        "        gif_path (str): Path to save GIF\n",
        "        show (bool): Whether to display GIF in notebook\n",
        "    \"\"\"\n",
        "    # Downsample for speed\n",
        "    traj_ds = traj[::downsample_points, ::downsample_frames, :]\n",
        "    num_points, num_timesteps, _ = traj_ds.shape\n",
        "\n",
        "    # Viridis colormap\n",
        "    colors = cm.viridis(np.linspace(0, 1, num_points))\n",
        "\n",
        "    # Set up the plot\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    scat = ax.scatter(traj_ds[:, 0, 0], traj_ds[:, 0, 1], s=5, c=colors)\n",
        "    ax.set_xlim(np.min(traj_ds[:, :, 0]), np.max(traj_ds[:, :, 0]))\n",
        "    ax.set_ylim(np.min(traj_ds[:, :, 1]), np.max(traj_ds[:, :, 1]))\n",
        "    ax.set_title(f\"Timestep: 0\")\n",
        "\n",
        "    # Animation update function\n",
        "    def update(frame):\n",
        "        scat.set_offsets(traj_ds[:, frame, :])\n",
        "        ax.set_title(f\"Timestep: {frame * downsample_frames}\")\n",
        "        return scat,\n",
        "\n",
        "    # Create animation (does NOT repeat automatically)\n",
        "    anim = FuncAnimation(\n",
        "        fig,\n",
        "        update,\n",
        "        frames=num_timesteps,\n",
        "        interval=20,\n",
        "        blit=True,\n",
        "        repeat=False\n",
        "    )\n",
        "\n",
        "    # Save as GIF\n",
        "    anim.save(gif_path, writer=PillowWriter(fps=30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAsEPht8QZyV"
      },
      "outputs": [],
      "source": [
        "batch_size = 2048\n",
        "num_epochs = 1024\n",
        "lr = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5kYGqyoQ6h8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(f\"cuda:{0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdHJA9G9QZyV"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = TensorDataset(torch.from_numpy(X).float())\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL1-4r8IQZyW"
      },
      "source": [
        "## Part I: Fundamentals of Continuous-Time Diffusion Models\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VjNsv8dQZyW"
      },
      "source": [
        "### Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aBdxjH7QZyW"
      },
      "outputs": [],
      "source": [
        "plot_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHDOk30tQZyW"
      },
      "source": [
        "### Forward process\n",
        "\n",
        "The forward process describes how clean data $\\mathbf{x}_0$ is gradually perturbed with noise over (continous) time. It is formally specified as a stochastic differential equation.\n",
        "There are many possible choices, but we focus on so called **variance-preserving SDE**,\n",
        "\n",
        "$$\n",
        "\\mathrm{d}\\mathbf{x} = -\\tfrac{1}{2} \\beta(t) \\mathbf{x} \\, \\mathrm{d}t + \\sqrt{\\beta(t)} \\, \\mathrm{d}\\mathbf{w},\n",
        "$$\n",
        "\n",
        "where $\\beta(t)$ controls the noise rate and $\\mathrm{d}\\mathbf{w}$ is standard Brownian motion.\n",
        "\n",
        "The forward process plays the role of **progressively destroying structure in the data** by injecting Gaussian noise in a controlled manner.\n",
        "\n",
        "- At **early times** ($t \\approx 0$), $\\mathbf{x}_t$ is close to the original data.\n",
        "- At **later times** ($t \\to 1$), $\\mathbf{x}_t$ becomes nearly pure Gaussian noise.\n",
        "\n",
        "This gradual corruption provides an _interpolation_ between the complex data distribution and a simple Gaussian prior. The reverse process then learns to recover the clean part from the interpolation during generation.\n",
        "\n",
        "We will use the following $\\beta(t)$ schedule,\n",
        "\n",
        "$$\n",
        "\\beta(t) = \\beta_{\\min} + t \\, (\\beta_{\\max} - \\beta_{\\min}), \\quad t \\in [0, 1]\n",
        "$$\n",
        "\n",
        "Analytical solution to our (forward) SDE is $ X_t = \\mathcal{N}(\\mathbf{X}\\_t; \\, \\alpha(t)\\mathbf{x}\\_0, \\, \\sigma^2(t)\\mathbf{I}) $, where\n",
        "\n",
        "1.  $\n",
        "  \\alpha(t) = \\exp\\!\\Big(-\\tfrac{1}{2} \\int_0^t \\beta(s) \\, \\mathrm{d}s\\Big),\n",
        "  $\n",
        "2.  $\n",
        "  \\sigma^2(t) = 1 - \\alpha(t)^2,\n",
        "  $\n",
        "\n",
        "and $\\mathbf{I}$ is the identity matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI8odAXRQZyW"
      },
      "source": [
        "#### Task: Implement the forward process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jklm2NeUQZyW"
      },
      "outputs": [],
      "source": [
        "def beta(t, beta_min=0.1, beta_max=20):\n",
        "    \"\"\"\n",
        "    Linear beta schedule for VP-SDE.\n",
        "\n",
        "    Args:\n",
        "        t: torch.Tensor of shape (...), values in [0,1]\n",
        "        beta_min: minimum beta\n",
        "        beta_max: maximum beta\n",
        "\n",
        "    Returns:\n",
        "        beta(t): torch.Tensor of same shape as t\n",
        "    \"\"\"\n",
        "    \"YOUR CODE GOES HERE\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "def alpha(t, beta_min=0.1, beta_max=20):\n",
        "    \"\"\"\n",
        "    Compute alpha(t) for VP-SDE with linear beta schedule.\n",
        "\n",
        "    Args:\n",
        "        t: torch.Tensor of shape (...), values in [0,1]\n",
        "\n",
        "    Returns:\n",
        "        alpha: torch.Tensor same shape as t\n",
        "    \"\"\"\n",
        "    \"YOUR CODE GOES HERE\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "def sigma(t, beta_min=0.1, beta_max=20):\n",
        "    \"\"\"\n",
        "    Compute sigma(t) for VP-SDE with linear beta schedule.\n",
        "\n",
        "    Args:\n",
        "        t: torch.Tensor of shape (...), values in [0,1]\n",
        "\n",
        "    Returns:\n",
        "        sigma: torch.Tensor same shape as t\n",
        "    \"\"\"\n",
        "    \"YOUR CODE GOES HERE\"\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I9vs2k1QZyW"
      },
      "outputs": [],
      "source": [
        "plot_schedule(alpha, beta, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y--90O_PQZyW"
      },
      "source": [
        "Question: How do beta, alpha and sigma behave as time goes from 0 to 1. What do samples from p(x_t | x_0) look like?\n",
        "Answers: Beta increases linearly, alpha gradually decreases -> less and less signal, while sigma gradually increases more and more noise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dwAlcHWGQZyW",
        "outputId": "fa296f33-9f2a-4574-c0f2-89103075867e"
      },
      "outputs": [],
      "source": [
        "plot_forward(X, y, alpha, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o61GyHwQZyW"
      },
      "source": [
        "### Score Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFfrhCcJQZyW"
      },
      "source": [
        "#### Task: Implement a Time-Conditioned MLP (ScoreModel)\n",
        "\n",
        "Implement a PyTorch model called `ScoreModel` that predicts a vector of the same dimension as its input, conditioned on both an input vector `x` and a scalar time `t`. The model should use a multi-layer perceptron (MLP) architecture with a configurable number of hidden layers and hidden units, using SiLU activation.\n",
        "\n",
        "Specification:\n",
        "\n",
        "1. **Inputs:**\n",
        "\n",
        "   - `x`: Tensor of shape `(batch_size, dim)`.\n",
        "   - `t`: Tensor of shape `(batch_size,)` representing a scalar time for each sample.\n",
        "\n",
        "2. **Output:**\n",
        "\n",
        "   - Tensor of shape `(batch_size, dim)`.\n",
        "\n",
        "3. **Model Architecture:**\n",
        "\n",
        "   - The model is an MLP that:\n",
        "\n",
        "     1. Concatenates `x` and `t` along the last dimension to form a tensor of shape `(batch_size, dim + 1)`.\n",
        "     2. Passes this through an **input linear layer** projecting `(dim + 1) → hidden_size`.\n",
        "     3. Applies the **SiLU activation**.\n",
        "     4. Passes through `num_blocks` **hidden layers**, each consisting of:\n",
        "        - Linear layer: `hidden_size → hidden_size`\n",
        "        - SiLU activation\n",
        "     5. Passes through a **final linear layer** projecting `hidden_size → dim`.\n",
        "\n",
        "**Example Usage:**\n",
        "\n",
        "```python\n",
        "dim = 2\n",
        "model = ScoreModel(dim=dim, num_blocks=3, hidden_size=128)\n",
        "\n",
        "x = torch.randn(16, dim)  # batch of 16 samples\n",
        "t = torch.randn(16)       # corresponding times\n",
        "\n",
        "output = model(x, t)      # shape: (16, 2)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6xFGg3vQZyX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ScoreModel(nn.Sequential):\n",
        "    def __init__(self, dim: int, num_blocks: int = 2, hidden_size: int = 64):\n",
        "        \"YOUR CODE GOES HERE\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "        \"YOUR CODE GOES HERE\"\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtIwh3KJQZyX"
      },
      "outputs": [],
      "source": [
        "model = ScoreModel(dim=2, hidden_size=256).to(device)\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXRa3gCZQZyX"
      },
      "source": [
        "### Training with Denoising Score Matching\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7knkL49QZyX"
      },
      "source": [
        "We will train our score model with Denoising Score Matching (DSM).\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{x}_0, t, \\mathbf{x}_t} \\Big[ \\lambda(t) \\, \\big\\| s_\\theta(\\mathbf{x}_t, t) - \\nabla_{\\mathbf{x}_t} \\log p_{t}(\\mathbf{x}_t \\mid \\mathbf{x}_0) \\big\\|_2^2 \\Big],\n",
        "$$\n",
        "\n",
        "where $\\lambda(t)$ balances contributions across noise levels.\n",
        "\n",
        "For numerical stability, we optimize\n",
        "$$\\mathcal{L}(\\theta) = \\mathbb{E}_{t \\sim \\mathcal{U}(0,1)} \\left[ \\mathbb{E}_{\\mathbf{x}_0 \\sim p_0(\\mathbf{x}_0)}  \\sigma(t)^2 \\left[ \\| \\mathbf{s}_\\theta(\\mathbf{x}_t, t) + (\\mathbf{x}_t - \\alpha(t)\\mathbf{x}_0)  \\|_2^2 \\right] \\right]$$\n",
        "\n",
        "We will estimate $\\mathcal{L}(\\theta)$ with a simple Monte-Carlo estimator\n",
        "\n",
        "$$\n",
        "\\widehat{\\mathcal{L}}(\\theta) \\;=\\;\n",
        "\\frac{1}{B} \\sum_{i=1}^{B}\n",
        " \\sigma(t^{(i)})^2\n",
        "\\left\\|\\, s_\\theta\\!\\big(\\mathbf{x}_t^{(i)}, t^{(i)}\\big)\n",
        "\\;+\\;  (\\mathbf{x}_t^{(i)} - \\alpha(t^{(i)})\\mathbf{x}_0^{(i)})  ,\\right\\|_2^{2},\n",
        "$$\n",
        "\n",
        "where each triplet $(\\mathbf{x}_0^{(i)}, t^{(i)}, \\mathbf{x}_t^{(i)})$ is drawn by:\n",
        "\n",
        "- sampling a clean datapoint $\\mathbf{x}_0^{(i)} \\sim p_{\\text{data}}$,\n",
        "- sampling a time $t^{(i)} \\sim \\operatorname{Uniform}([0,1]) $,\n",
        "- sampling $\\mathbf{x}_t^{(i)} \\sim p_t(\\mathbf{x}_t \\mid \\mathbf{x}_0^{(i)})$, **using the closed-form solution** of our forward SDE.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC7FCZ9lQZyX"
      },
      "source": [
        "#### Task: Implement the training step\n",
        "\n",
        "Based on the provided Monte-Carlo estimator for the Denoising Score Matching loss, please complete the `training_step` function.\n",
        "\n",
        "**Inputs:**\n",
        "\n",
        "- `model`: An instance of `ScoreModel`, which is the neural network being trained.\n",
        "\n",
        "- `x`: A `torch.Tensor` representing the batch of clean data points.\n",
        "\n",
        "**Output:**\n",
        "\n",
        "- A `torch.Tensor` representing the calculated loss value for the batch.\n",
        "\n",
        "Your implementation should use previously implemented `alpha` and `sigma` functions to:\n",
        "\n",
        "1. Sample a random time `t` and a random noise vector `z`.\n",
        "\n",
        "2. Perturb the clean data `x` to create the noisy data `x_t`.\n",
        "\n",
        "3. Compute the target score, which for this model is a scaled version of the actual score.\n",
        "\n",
        "4. Return the Mean Squared Error (MSE) between the model's predicted score and the target score, averaged over both the input dimensions and the batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CheSjWSmQZyX"
      },
      "outputs": [],
      "source": [
        "def training_step(model: ScoreModel, x: torch.Tensor) -> torch.Tensor:\n",
        "    # x: (b, 2) [points]\n",
        "    b, *_ = x.shape\n",
        "    \"YOUR CODE GOES HERE\"\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkM1vsTxQZyX"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mAriSPjQZyX"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "log_every_steps = 128\n",
        "total_epochs = 0\n",
        "\n",
        "# Training loop with tqdm\n",
        "for epoch in range(num_epochs):\n",
        "    step = 0\n",
        "    # Wrap the DataLoader with tqdm for a progress bar\n",
        "    progress_loader = tqdm(\n",
        "        loader,\n",
        "        desc=f'epoch [{epoch+1}]',\n",
        "        leave=False\n",
        "    )\n",
        "    for x, in progress_loader:\n",
        "        # Move inputs to the device\n",
        "        x = x.to(device)\n",
        "        # Compute the loss\n",
        "        loss = training_step(model, x)\n",
        "        loss.backward()\n",
        "        # Clip gradient norm\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            model.parameters(),\n",
        "            max_norm=1.0\n",
        "        )\n",
        "        optimizer.step()\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        if step % log_every_steps == 0:\n",
        "            progress_loader.set_postfix(loss=loss.item())\n",
        "        step += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTZFoaU5QmQR"
      },
      "source": [
        "### Sampling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12mvIjBLQZyX"
      },
      "source": [
        "#### Task: Sampling from a Variance-Preserving SDE using Euler-Maruyama\n",
        "\n",
        "Recall that the reverse-time SDE is defined as:\n",
        "\n",
        "$$\n",
        "dx = \\Big[-\\frac{1}{2}\\beta(t)x - \\beta(t)s_\\theta(x,t)\\Big]\\,dt + \\sqrt{\\beta(t)}\\,d\\bar{w}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $x$ is the system state at time $t$, flowing **backwards from $T=1$ down to $\\epsilon=1\\text{e-5}$**. We **stop at $\\epsilon$ instead of 0 for numerical stability**.\n",
        "- $\\beta(t)$ is the variance schedule function.\n",
        "- $s_\\theta(x,t)$ is the score function estimated by the model.\n",
        "- $d\\bar{w}$ is standard Brownian motion in reverse time.\n",
        "\n",
        "  **Note:** the model is trained to regress $\\sigma^2(t) s_\\theta(x,t)$, so you need to **scale the model output by $1/\\sigma^2(t)$** to get the actual score.\n",
        "\n",
        "We discretize this SDE with **Euler-Maruyama** as:\n",
        "\n",
        "$$\n",
        "x_{t-\\Delta t} = x_t - \\Big[-\\frac{1}{2}\\beta(t)x_t - \\beta(t)s_\\theta(x_t,t)\\Big]\\Delta t + \\sqrt{\\beta(t)\\Delta t}\\,z\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $t$ is the current time step.\n",
        "- $x_{t-\\Delta t}$ is the state at the next (earlier) time step.\n",
        "- $\\Delta t = t_i - t_{i+1}$ is the positive step size.\n",
        "- $s_\\theta(x_t,t)$ is the estimated score function.\n",
        "- $z$ is a standard Gaussian noise vector, corresponding to $d\\bar{w}$.\n",
        "\n",
        "**Your task:**\n",
        "\n",
        "Implement a PyTorch function `sample_vp_sde_euler_maruyama` that:\n",
        "\n",
        "1. **Inputs:**\n",
        "\n",
        "   - `model: ScoreModel` – a PyTorch module that takes `(x, t)` and returns the **scaled score** $\\sigma^2(t) s_\\theta(x,t)$.\n",
        "   - `num_samples: int` – number of samples to generate.\n",
        "   - `sample_shape: List[int]` – shape of a single sample (default `[2]`).\n",
        "   - `T: float` – starting time (default `1.0`).\n",
        "   - `num_steps: int` – number of discrete steps for Euler-Maruyama (default `1000`).\n",
        "   - `eps: float` – final time for numerical stability (default `1e-5`).\n",
        "   - `device: str` – device for computation (CPU or GPU).\n",
        "\n",
        "2. **Outputs:**\n",
        "\n",
        "   - `x_T: torch.Tensor` – final state at $t \\approx \\epsilon$, shape `[num_samples] + sample_shape`.\n",
        "   - `trajectory: torch.Tensor` – the full trajectory of all samples, shape `[num_samples, num_steps+1, *sample_shape]`.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "- Use `torch.full` to create a time tensor for the batch and `torch.randn_like` for the Brownian noise.\n",
        "- Remember to **divide the model output by $\\sigma^2(t)$** to obtain the true score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUKGFutNQZyX"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "import torch\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def sample_vp_sde_euler_maruyama(\n",
        "    model: ScoreModel,\n",
        "    num_samples: int,\n",
        "    sample_shape: List[int] = [2],\n",
        "    T=1.0,\n",
        "    num_steps=1000,\n",
        "    eps=1e-5,\n",
        "    device='cpu',\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    # Create time steps from T down to eps, for numerical stability\n",
        "    t_steps = torch.linspace(T, eps, num_steps + 1, device=device)\n",
        "    # Start with a random noise vector for all samples\n",
        "    x = torch.randn([num_samples]+sample_shape, device=device)\n",
        "    # List to store the trajectory of each sample\n",
        "    trajectory = [x.clone()]\n",
        "    for i in range(num_steps):\n",
        "        t = t_steps[i]\n",
        "        \"YOUR CODE GOES HERE\"\n",
        "        raise NotImplementedError()\n",
        "    # Rearrange trajectory\n",
        "    trajectory = (\n",
        "        torch\n",
        "        .stack(trajectory, dim=0)\n",
        "        .permute(1, 0, 2)\n",
        "    )\n",
        "    return (\n",
        "        trajectory[:, -1, :],\n",
        "        trajectory\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXpvL5RUQZyX"
      },
      "outputs": [],
      "source": [
        "x, traj = sample_vp_sde_euler_maruyama(\n",
        "    model,\n",
        "    num_samples=1000,\n",
        "    num_steps=1000,\n",
        "    device=device\n",
        ")\n",
        "x = x.detach().cpu().numpy()\n",
        "traj = traj.detach().cpu().numpy()\n",
        "\n",
        "x.shape, traj.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46uvvQsVQZyX"
      },
      "source": [
        "**Denoising animation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHVNh7SKQZyX"
      },
      "outputs": [],
      "source": [
        "animate_trajectories(traj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "TyxzlDpyQZyX",
        "outputId": "44c84a2a-9769-4abc-dd2f-298c244366ce"
      },
      "outputs": [],
      "source": [
        "display(Image(filename=\"denoising.gif\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AjXbbm6QZyX"
      },
      "source": [
        "**Denoising summary**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reX3hb2wQZyX"
      },
      "outputs": [],
      "source": [
        "plot_denoising_trajectories(traj, X)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}